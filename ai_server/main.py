from fastapi import FastAPI, UploadFile, File, Form
from fastapi.responses import JSONResponse
import whisper
import shutil
import os
import re
import traceback
import subprocess

app = FastAPI()

# ëª¨ë¸ ë¡œë“œ
print("â³ ëª¨ë¸ ë¡œë”© ì¤‘... (Small)")
model = whisper.load_model("small")
print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")

# ğŸ› ï¸ [ê¸°ì¡´ ìœ ì§€] FFmpegê°€ ì–´ë””ì— ìˆë“  ì°¾ì•„ë‚´ëŠ” ë˜‘ë˜‘í•œ í•¨ìˆ˜
def get_ffmpeg_command():
    # 1. Chocoë¡œ ì„¤ì¹˜ëœ(ì‹œìŠ¤í…œì— ê¹”ë¦°) ffmpegê°€ ìˆëŠ”ì§€ í™•ì¸
    if shutil.which("ffmpeg"):
        print("ğŸ”§ [ë„êµ¬ ë°œê²¬] ì‹œìŠ¤í…œ ê¸°ë³¸(Choco ë“±) ffmpeg ì‚¬ìš©")
        return "ffmpeg"
    
    # 2. ì—†ë‹¤ë©´, ë‚´ í´ë”(ai_server) ì•ˆì— exeê°€ ìˆëŠ”ì§€ í™•ì¸
    current_dir = os.path.dirname(os.path.abspath(__file__))
    local_ffmpeg = os.path.join(current_dir, "ffmpeg.exe")
    
    if os.path.exists(local_ffmpeg):
        print(f"ğŸ”§ [ë„êµ¬ ë°œê²¬] ë¡œì»¬ í´ë” ë‚´ ffmpeg ì‚¬ìš©: {local_ffmpeg}")
        return local_ffmpeg
    
    # 3. ë‘˜ ë‹¤ ì—†ìœ¼ë©´ ì—ëŸ¬
    raise FileNotFoundError("FFmpegë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (Choco ì„¤ì¹˜ ë˜ëŠ” exe íŒŒì¼ ë³µì‚¬ í•„ìš”)")

def convert_to_clean_wav(input_path):
    try:
        command_executable = get_ffmpeg_command()
        output_path = os.path.splitext(input_path)[0] + "_clean.wav"
        print(f"ğŸ”„ [ë³€í™˜ ì‹œì‘] {input_path} -> {output_path}")
        
        command = [
            command_executable, 
            "-i", input_path,
            "-ar", "16000",
            "-ac", "1",
            "-c:a", "pcm_s16le",
            "-vn",
            "-y",
            output_path
        ]
        
        subprocess.run(command, check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
        
        print("âœ… [ë³€í™˜ ì™„ë£Œ] ê¹¨ë—í•œ WAV íŒŒì¼ ìƒì„±ë¨")
        return output_path

    except Exception as e:
        print(f"ğŸš¨ ë³€í™˜ ì‹¤íŒ¨ (ì›ë³¸ ì‚¬ìš©): {e}")
        return input_path 

# ğŸ› ï¸ [ì¶”ê°€] AIê°€ ë±‰ì€ í™˜ê°(Lyrics, MBC ë“±) ì²­ì†Œí•˜ëŠ” í•¨ìˆ˜
def clean_hallucinations(segments):
    cleaned = []
    # ì§€ì›Œë²„ë¦´ ê¸ˆì§€ì–´ ë¦¬ìŠ¤íŠ¸ (ì†Œë¬¸ìë¡œ ì‘ì„±)
    banned_words = ["lyrics", "lyrics.", "ë…¸ë˜ ê°€ì‚¬", "mbc", "subtitles", "sous-titres", "ì‹œì²­í•´ ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤"]
    
    for seg in segments:
        text = seg['text'].strip()
        
        # 1. ë‚´ìš©ì´ ì•„ì˜ˆ ì—†ìœ¼ë©´ íŒ¨ìŠ¤
        if not text: continue
        
        # 2. ê¸ˆì§€ì–´ì™€ ë˜‘ê°™ìœ¼ë©´ íŒ¨ìŠ¤ (ëŒ€ì†Œë¬¸ì ë¬´ì‹œ)
        if text.lower() in banned_words:
            continue
            
        # 3. íŠ¹ìˆ˜ë¬¸ìë§Œ ìˆëŠ” ê²½ìš° íŒ¨ìŠ¤ (ì˜ˆ: "...")
        if re.match(r'^[\W_]+$', text):
            continue

        cleaned.append(seg)
        
    return cleaned

# 1. ì¼ë°˜ì ì¸ LRC íŒŒì‹±
def parse_lrc_with_timestamp(lrc_content: str):
    segments = []
    pattern = re.compile(r'\[?(\d+):(\d+\.?\d*)\]?\s*(.*)')
    
    for line in lrc_content.splitlines():
        line = line.strip()
        if not line: continue
        match = pattern.match(line)
        if match:
            minutes = int(match.group(1))
            seconds = float(match.group(2))
            text = match.group(3).strip()
            total_seconds = minutes * 60 + seconds
            if text:
                segments.append({"start": total_seconds, "text": text})
    return segments

# 2. ê°•ì œ ì‹±í¬ ë§ì¶¤
def force_align_lyrics(whisper_result, user_text):
    ai_timestamps = [seg['start'] for seg in whisper_result['segments']]
    user_lines = [line.strip() for line in user_text.splitlines() if line.strip()]
    
    if not ai_timestamps or not user_lines: return []

    final_segments = []
    if not whisper_result['segments']: return [] 

    total_ai_duration = whisper_result['segments'][-1]['end'] - whisper_result['segments'][0]['start']
    start_offset = whisper_result['segments'][0]['start']
    count = len(user_lines)
    
    for i, line in enumerate(user_lines):
        percent = i / count 
        calculated_time = start_offset + (total_ai_duration * percent)
        calculated_time = round(calculated_time, 2)
        final_segments.append({"start": calculated_time, "text": line})
        
    return final_segments

@app.post("/analyze")
async def analyze_audio(
    file: UploadFile = File(...), 
    language: str = Form("auto"), 
    lyrics_text: str = Form(None)
):
    temp_filename = f"temp_{file.filename}"
    clean_audio_path = None
    
    actual_language = None if language == "auto" else language

    print(f"\nğŸš€ [ìš”ì²­] {file.filename} / ì–¸ì–´: {actual_language if actual_language else 'ìë™'}")

    try:
        # 1. ì›ë³¸ ì €ì¥
        with open(temp_filename, "wb") as buffer:
            shutil.copyfileobj(file.file, buffer)

        # 2. ë³€í™˜ ì‹œë„ (Choco or ë¡œì»¬ íŒŒì¼)
        clean_audio_path = convert_to_clean_wav(temp_filename)

        # A. ì‚¬ìš©ì ê°€ì‚¬ ìˆìŒ
        if lyrics_text:
            print(f"ğŸ“ ì‚¬ìš©ì ê°€ì‚¬ ìˆ˜ì‹ ë¨ (ê¸¸ì´: {len(lyrics_text)})")
            
            parsed = parse_lrc_with_timestamp(lyrics_text)
            if len(parsed) > 0:
                print("âœ¨ ì‹œê°„ ì •ë³´ í¬í•¨ë¨ -> ë°”ë¡œ ì ìš©")
                if os.path.exists(temp_filename): os.remove(temp_filename)
                if clean_audio_path != temp_filename and os.path.exists(clean_audio_path): 
                    os.remove(clean_audio_path)
                return JSONResponse(content={"segments": parsed})
            
            print("ğŸ’¡ í…ìŠ¤íŠ¸ë§Œ ìˆìŒ -> AIë¡œ ì‹œê°„ ì¶”ì¶œ")
            raw_result = model.transcribe(clean_audio_path, language=actual_language, fp16=False)
            aligned_result = force_align_lyrics(raw_result, lyrics_text)
            
            print(f"âœ… ë§¤í•‘ ì™„ë£Œ: ì´ {len(aligned_result)}ì¤„")
            
            if os.path.exists(temp_filename): os.remove(temp_filename)
            if clean_audio_path != temp_filename and os.path.exists(clean_audio_path): 
                os.remove(clean_audio_path)
            return JSONResponse(content={"segments": aligned_result})

        # B. ê°€ì‚¬ ì—†ìŒ (í™˜ê° ë°©ì§€ ê¸°ëŠ¥ ì¶”ê°€ë¨)
        else:
            print(f"ğŸ¤– ê°€ì‚¬ ì—†ìŒ -> AI ë°›ì•„ì“°ê¸° ëª¨ë“œ")
            
            # â¬‡ï¸ [ìˆ˜ì •] í™˜ê° ë°©ì§€ ì˜µì…˜ ì ìš©
            result = model.transcribe(
                clean_audio_path, 
                language=actual_language,
                initial_prompt="Hello, this is a song.", # íŒíŠ¸ ë³€ê²½
                fp16=False,
                condition_on_previous_text=False, # ì•µë¬´ìƒˆ ë°©ì§€
                no_speech_threshold=0.6, # ì¡ìŒ ë¬´ì‹œ
                logprob_threshold=-1.0   # í™•ì‹  ì—†ìœ¼ë©´ ë²„ë¦¼
            )

            # â¬‡ï¸ [ì¶”ê°€] ì“°ë ˆê¸° ê°’ ì²­ì†Œ
            result['segments'] = clean_hallucinations(result['segments'])
            
            if os.path.exists(temp_filename): os.remove(temp_filename)
            if clean_audio_path != temp_filename and os.path.exists(clean_audio_path): 
                os.remove(clean_audio_path)
                
            return JSONResponse(content=result)

    except Exception as e:
        print(f"\nğŸ’¥ ì—ëŸ¬ ë°œìƒ: {traceback.format_exc()}")
        if os.path.exists(temp_filename): os.remove(temp_filename)
        if clean_audio_path and clean_audio_path != temp_filename and os.path.exists(clean_audio_path): 
            os.remove(clean_audio_path)
        return JSONResponse(content={"error": str(e)}, status_code=500)